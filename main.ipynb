{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tomertech/BiLSTMDependencyParser/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rUPr9jrr6Jbv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ytISs8sQ6d4z"
      },
      "outputs": [],
      "source": [
        "is_colab=True\n",
        "pres_of_1=0.104571953\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Model\n",
        "input_dim = 3  # number of input features\n",
        "hidden_dim = 128  # number of hidden units in LSTM layer\n",
        "output_dim = 1  # number of output classes\n",
        "num_layers = 2  # number of LSTM layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3QB-GQ16T_x",
        "outputId": "862e387d-3726-47e0-98d3-65c4d3dba958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "if is_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    GDRIVE_DIR = '/content/gdrive/MyDrive/Technion/Cognition/project'\n",
        "else:\n",
        "    GDRIVE_DIR = './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "64lBPSb36KmM"
      },
      "outputs": [],
      "source": [
        "def open_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_pickle(df, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(df, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JredhdQY6E0z"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(df):\n",
        "\n",
        "    df['label'] = df['L1'].apply(lambda x: x == \"English\").astype(int)\n",
        "    df['CURRENT_FIX_INTEREST_AREA_ID'] = df['CURRENT_FIX_INTEREST_AREA_ID'].drop(\n",
        "        df[df['CURRENT_FIX_INTEREST_AREA_ID'] == '.'].index)\n",
        "    df = df.dropna(subset=['CURRENT_FIX_DURATION', 'CURRENT_FIX_INTEREST_AREA_ID', 'CURRENT_FIX_X'])\n",
        "    df['CURRENT_FIX_INTEREST_AREA_ID'] = df['CURRENT_FIX_INTEREST_AREA_ID'].astype(int)\n",
        "\n",
        "    # Normalize the data with z-score normalization\n",
        "    df['CURRENT_FIX_DURATION'] = (df['CURRENT_FIX_DURATION'] - df['CURRENT_FIX_DURATION'].mean()) / df[\n",
        "        'CURRENT_FIX_DURATION'].std()\n",
        "    df['CURRENT_FIX_INTEREST_AREA_ID'] = (df['CURRENT_FIX_INTEREST_AREA_ID'] - df[\n",
        "        'CURRENT_FIX_INTEREST_AREA_ID'].mean()) / df['CURRENT_FIX_INTEREST_AREA_ID'].std()\n",
        "    df['CURRENT_FIX_X'] = (df['CURRENT_FIX_X'] - df['CURRENT_FIX_X'].mean()) / df['CURRENT_FIX_X'].std()\n",
        "\n",
        "    sentences_gpby = df.groupby(['list', 'sentenceid'])\n",
        "    sentences, ys = [], []\n",
        "\n",
        "    for _, group in sentences_gpby:\n",
        "        sentences.append(torch.tensor(group[['CURRENT_FIX_DURATION', 'CURRENT_FIX_INTEREST_AREA_ID', 'CURRENT_FIX_X']].values).float().squeeze(1))\n",
        "        ys.append(torch.tensor(group['label'].iloc[0]).long())\n",
        "    return sentences, ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vd8c8XHO6lzi"
      },
      "outputs": [],
      "source": [
        "df = open_pickle(f'{GDRIVE_DIR}/data/features_base.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "POidM_806qRC"
      },
      "outputs": [],
      "source": [
        "data_dir= f'{GDRIVE_DIR}/data/'\n",
        "filename_s = data_dir+'sentences.pkl'\n",
        "filename_y = data_dir+'ys.pkl'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(filename_s):\n",
        "    # If the file exists, load the data\n",
        "    with open(filename_s, 'rb') as f:\n",
        "        sentences = pickle.load(f)\n",
        "    with open(filename_y, 'rb') as f:\n",
        "        ys = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    # If the file doesn't exist, run your data preprocessing function\n",
        "    sentences, ys = data_preprocessing(df)\n",
        "    \n",
        "    # And then save the data to a file for future use\n",
        "    with open(filename_s, 'wb') as f:\n",
        "        pickle.dump(sentences, f)\n",
        "    with open(filename_y, 'wb') as f:\n",
        "        pickle.dump(ys, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COLAB_NUM_OF_EXAMPLES = 10000\n",
        "if is_colab:\n",
        "    sentences, ys = sentences[:COLAB_NUM_OF_EXAMPLES], ys[:COLAB_NUM_OF_EXAMPLES]"
      ],
      "metadata": {
        "id": "ICOUnW4B6d44"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LRbZR7i-nRR",
        "outputId": "1eeb538b-1a15-45e3-d3d4-3be076777623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def collate_fn_padd(batch):\n",
        "\n",
        "    ## get sequence lengths\n",
        "    X, y = zip(*batch)\n",
        "    X_lengths = torch.tensor([x.shape[0] for x in X]).to(device)\n",
        "\n",
        "    ## padd\n",
        "    X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True).to(device)\n",
        "    # y (labels) need to be (N,C) shape, stated here: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss\n",
        "    y = torch.stack(y).to(device)\n",
        "    return X.float(), y.float(), X_lengths.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xQkuIx2q6d46"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# weights = [pres_of_1,1-pres_of_1]\n",
        "# class_weights = torch.FloatTensor(weights).to(device)\n",
        "# criterion = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6yUtvLS06wAP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# Create a dataset from tensors directly\n",
        "dataset = list(zip(sentences, torch.tensor(ys).unsqueeze(dim=1)))\n",
        "\n",
        "# Define the split sizes (e.g., 70% train, 15% valid, 15% test)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "valid_size = (len(dataset) - train_size) // 2\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "\n",
        "# Split dataset\n",
        "train_data, valid_data, test_data = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn_padd)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_fn_padd)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn_padd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "58T_ggWujld7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1lGH4EN9kqY",
        "outputId": "a20135a4-079c-405b-c3b8-3ff1df3415d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc_1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]  # we are taking the last output of the sequence\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc_2(self.activation(self.fc_1(out)))\n",
        "        out = torch.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jHs3KdooAsHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f89564fd-f2af-42a5-ef11-4497848f011c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of LSTMClassifier(\n",
              "  (lstm): LSTM(3, 128, num_layers=2, batch_first=True)\n",
              "  (fc_1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc_2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (activation): ReLU()\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "model = LSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim,\n",
        "                       output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3)\n",
        "model.parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8ZPpMFCssM_R"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Ensure the model is in training mode\n",
        "        total_loss = 0.0\n",
        "        for i, (inputs, labels, lengths) in enumerate(train_dataloader):\n",
        "            # Forward pass\n",
        "            predictions = model(inputs)\n",
        "            predicted = (predictions > 0.5).float()\n",
        "            # if predicted.sum(): print(f\"iteration: {i}; predictions: {predicted.sum()}, labels: {labels.sum()}\")\n",
        "            # print(f\"predictions.shape: {predictions.shape}\")\n",
        "            # print(f\"inputs.shape: {inputs.shape}\")\n",
        "            loss = criterion(predictions, labels)\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {round(total_loss/(len(train_dataloader)), 3)}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, lengths in val_dataloader:\n",
        "                predictions = model(inputs)\n",
        "                loss = criterion(predictions, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Adjusting this line to correctly threshold predictions\n",
        "                predicted = (predictions > 0.5).float()\n",
        "                # print(predicted)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_dataloader)\n",
        "        val_accuracy = correct / total * 100.0\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {round(val_loss, 3)}, Validation Accuracy: {round(val_accuracy, 3)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuLlZo8qBtba",
        "outputId": "0bfc93cf-03bc-46e2-8589-1e569af55aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Training Loss: 0.489\n",
            "Epoch 1/50, Validation Loss: 0.485, Validation Accuracy: 81.216%\n",
            "Epoch 2/50, Training Loss: 0.487\n",
            "Epoch 2/50, Validation Loss: 0.484, Validation Accuracy: 81.216%\n",
            "Epoch 3/50, Training Loss: 0.485\n",
            "Epoch 3/50, Validation Loss: 0.485, Validation Accuracy: 81.216%\n",
            "Epoch 4/50, Training Loss: 0.484\n",
            "Epoch 4/50, Validation Loss: 0.483, Validation Accuracy: 81.216%\n",
            "Epoch 5/50, Training Loss: 0.467\n",
            "Epoch 5/50, Validation Loss: 0.399, Validation Accuracy: 79.389%\n",
            "Epoch 6/50, Training Loss: 0.359\n",
            "Epoch 6/50, Validation Loss: 0.345, Validation Accuracy: 84.12%\n",
            "Epoch 7/50, Training Loss: 0.336\n",
            "Epoch 7/50, Validation Loss: 0.339, Validation Accuracy: 84.214%\n",
            "Epoch 8/50, Training Loss: 0.327\n",
            "Epoch 8/50, Validation Loss: 0.327, Validation Accuracy: 85.127%\n",
            "Epoch 9/50, Training Loss: 0.321\n",
            "Epoch 9/50, Validation Loss: 0.326, Validation Accuracy: 85.338%\n",
            "Epoch 10/50, Training Loss: 0.314\n",
            "Epoch 10/50, Validation Loss: 0.326, Validation Accuracy: 85.455%\n",
            "Epoch 11/50, Training Loss: 0.308\n",
            "Epoch 11/50, Validation Loss: 0.318, Validation Accuracy: 85.806%\n",
            "Epoch 12/50, Training Loss: 0.304\n",
            "Epoch 12/50, Validation Loss: 0.318, Validation Accuracy: 86.111%\n",
            "Epoch 13/50, Training Loss: 0.3\n",
            "Epoch 13/50, Validation Loss: 0.318, Validation Accuracy: 86.087%\n",
            "Epoch 14/50, Training Loss: 0.297\n",
            "Epoch 14/50, Validation Loss: 0.314, Validation Accuracy: 86.263%\n",
            "Epoch 15/50, Training Loss: 0.292\n",
            "Epoch 15/50, Validation Loss: 0.306, Validation Accuracy: 86.146%\n",
            "Epoch 16/50, Training Loss: 0.287\n",
            "Epoch 16/50, Validation Loss: 0.309, Validation Accuracy: 86.24%\n",
            "Epoch 17/50, Training Loss: 0.284\n",
            "Epoch 17/50, Validation Loss: 0.314, Validation Accuracy: 85.994%\n",
            "Epoch 18/50, Training Loss: 0.283\n",
            "Epoch 18/50, Validation Loss: 0.307, Validation Accuracy: 85.865%\n",
            "Epoch 19/50, Training Loss: 0.277\n",
            "Epoch 19/50, Validation Loss: 0.315, Validation Accuracy: 86.041%\n",
            "Epoch 20/50, Training Loss: 0.273\n",
            "Epoch 20/50, Validation Loss: 0.313, Validation Accuracy: 86.052%\n",
            "Epoch 21/50, Training Loss: 0.276\n",
            "Epoch 21/50, Validation Loss: 0.318, Validation Accuracy: 85.97%\n",
            "Epoch 22/50, Training Loss: 0.272\n",
            "Epoch 22/50, Validation Loss: 0.317, Validation Accuracy: 86.064%\n",
            "Epoch 23/50, Training Loss: 0.265\n",
            "Epoch 23/50, Validation Loss: 0.315, Validation Accuracy: 86.31%\n",
            "Epoch 24/50, Training Loss: 0.262\n",
            "Epoch 24/50, Validation Loss: 0.323, Validation Accuracy: 86.462%\n",
            "Epoch 25/50, Training Loss: 0.261\n",
            "Epoch 25/50, Validation Loss: 0.311, Validation Accuracy: 86.146%\n",
            "Epoch 26/50, Training Loss: 0.256\n",
            "Epoch 26/50, Validation Loss: 0.314, Validation Accuracy: 86.474%\n",
            "Epoch 27/50, Training Loss: 0.255\n",
            "Epoch 27/50, Validation Loss: 0.319, Validation Accuracy: 86.158%\n",
            "Epoch 28/50, Training Loss: 0.254\n",
            "Epoch 28/50, Validation Loss: 0.344, Validation Accuracy: 86.286%\n",
            "Epoch 29/50, Training Loss: 0.256\n",
            "Epoch 29/50, Validation Loss: 0.33, Validation Accuracy: 86.029%\n",
            "Epoch 30/50, Training Loss: 0.249\n",
            "Epoch 30/50, Validation Loss: 0.324, Validation Accuracy: 86.392%\n",
            "Epoch 31/50, Training Loss: 0.244\n",
            "Epoch 31/50, Validation Loss: 0.351, Validation Accuracy: 86.31%\n",
            "Epoch 32/50, Training Loss: 0.245\n",
            "Epoch 32/50, Validation Loss: 0.329, Validation Accuracy: 86.322%\n",
            "Epoch 33/50, Training Loss: 0.248\n",
            "Epoch 33/50, Validation Loss: 0.313, Validation Accuracy: 86.181%\n",
            "Epoch 34/50, Training Loss: 0.241\n",
            "Epoch 34/50, Validation Loss: 0.321, Validation Accuracy: 86.345%\n",
            "Epoch 35/50, Training Loss: 0.235\n",
            "Epoch 35/50, Validation Loss: 0.343, Validation Accuracy: 86.181%\n",
            "Epoch 36/50, Training Loss: 0.236\n",
            "Epoch 36/50, Validation Loss: 0.348, Validation Accuracy: 86.052%\n",
            "Epoch 37/50, Training Loss: 0.233\n",
            "Epoch 37/50, Validation Loss: 0.346, Validation Accuracy: 86.216%\n",
            "Epoch 38/50, Training Loss: 0.229\n",
            "Epoch 38/50, Validation Loss: 0.343, Validation Accuracy: 86.099%\n",
            "Epoch 39/50, Training Loss: 0.23\n",
            "Epoch 39/50, Validation Loss: 0.329, Validation Accuracy: 86.111%\n",
            "Epoch 40/50, Training Loss: 0.227\n",
            "Epoch 40/50, Validation Loss: 0.362, Validation Accuracy: 86.158%\n",
            "Epoch 41/50, Training Loss: 0.223\n",
            "Epoch 41/50, Validation Loss: 0.365, Validation Accuracy: 85.841%\n",
            "Epoch 42/50, Training Loss: 0.227\n",
            "Epoch 42/50, Validation Loss: 0.451, Validation Accuracy: 81.216%\n",
            "Epoch 43/50, Training Loss: 0.362\n",
            "Epoch 43/50, Validation Loss: 0.345, Validation Accuracy: 84.635%\n",
            "Epoch 44/50, Training Loss: 0.326\n",
            "Epoch 44/50, Validation Loss: 0.316, Validation Accuracy: 85.502%\n",
            "Epoch 45/50, Training Loss: 0.309\n",
            "Epoch 45/50, Validation Loss: 0.315, Validation Accuracy: 85.689%\n",
            "Epoch 46/50, Training Loss: 0.301\n",
            "Epoch 46/50, Validation Loss: 0.304, Validation Accuracy: 85.994%\n",
            "Epoch 47/50, Training Loss: 0.321\n",
            "Epoch 47/50, Validation Loss: 0.308, Validation Accuracy: 85.994%\n",
            "Epoch 48/50, Training Loss: 0.295\n",
            "Epoch 48/50, Validation Loss: 0.3, Validation Accuracy: 86.38%\n",
            "Epoch 49/50, Training Loss: 0.287\n",
            "Epoch 49/50, Validation Loss: 0.297, Validation Accuracy: 86.708%\n",
            "Epoch 50/50, Training Loss: 0.283\n",
            "Epoch 50/50, Validation Loss: 0.317, Validation Accuracy: 85.115%\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_dataloader, valid_dataloader, criterion, optimizer, num_epochs=50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}